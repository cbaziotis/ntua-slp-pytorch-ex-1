import numpy


def tokenize(text, lowercase=True):
    pass


def vectorize(text, word2idx, max_length):
    """
    Covert array of tokens, to array of ids, with a fixed length
    and zero padding at the end
    Args:
        text (): the wordlist
        word2idx (): dictionary of word to ids
        max_length (): the maximum length of the input sequences

    Returns: zero-padded list of ids

    """
    pass
